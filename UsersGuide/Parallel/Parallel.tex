\section{Parallelization}
\label{sec:parallel}

\amrex\ uses a hybrid MPI + X approach to parallelism,
where  X = OpenMP for multicore machines, and CUDA/HIP/DCP++ for CPU/GPU systems.
The basic idea is that MPI is used to distribute individual boxes across
nodes while X is used to distribute the work in local boxes
within a node.  The OpenMP approach in \amrex\ is optionally
based on {\em tiling} the box-based data structures.  Both the tiling and
non-tiling approaches to work distribution are discussed below.

\subsection{\amrex's Non-Tiling Approach In C++}
\label{sec:amrex0}

At the highest abstraction level, we have {\tt MultiFab} (mulitple
{\tt FArrayBox}es).  A {\tt MultiFab} contains an array of {\tt
  Box}es (a {\tt Box} contains integers specifying the index space it
covers), including {\tt Box}es owned by other processors for the
purpose of communication, an array of MPI ranks specifying which MPI
processor owns each {\tt Box}, and an array of pointers to {\tt
  FArrayBox}es owned by this MPI processor.
A typical usage of {\tt MultiFab} is as follows,

\begin{lstlisting}
  for (MFIter mfi(mf); mfi.isValid(); ++mfi) // Loop over boxes
  {
    // Get the index space of this iteration
    const Box& box = mfi.validbox(); 

    // Get a reference to mf's data as a multidimensional array 
    auto& data = mf.array(mfi);

    // Loop over "box" to update data.
    // On CPU/GPU systems, this loop executes on the GPU
  }
\end{lstlisting}
A few comments about this code
\begin{itemize}
\item Here the iterator, {\tt mfi}, will perform the loop only over the
   boxes that are local to the MPI task.  If there are 3 boxes on the
   processor, then this loop has 3 iterations.
      
\item {\tt box} as returned from {\tt mfi.validbox()} does not include
   ghost cells.  We can get the indices of the valid zones as {\tt
   box.loVect} and {\tt box.hiVect}.
\end{itemize}


\subsection{\amrex's Current Tiling Approach In C++}
\label{sec:amrex1}

There are two types of tiling that people discuss.  In {\em logical
tiling}, the data storage in memory is unchanged from how we do things
now in pure MPI.  In a given box, the data region is stored
contiguously).  But when we loop in OpenMP over a box, the tiling
changes how we loop over the data.  The alternative is called {\em
separate tiling}---here the data storage in memory itself is changed
to reflect how the tiling will be performed.  This is not considered
in \amrex.

In our logical tiling approach, a box is logically split into tiles,
and a {\tt MFIter} loops over each tile in each box.  Note that the
non-tiling iteration approach can be considered as a special case of
tiling with the tile size equal to the box size.

\begin{lstlisting}
  bool tiling = true;
  for (MFIter mfi(mf,tiling); mfi.isValid(); ++mfi) // Loop over tiles
  {
    // Get the index space of this iteration
    const Box& box = mfi.tilebox(); 

    // Get a reference to mf's data as a multidimensional array 
    auto& data = mf.array(mfi);

    // Loop over "box" to update data.
  }
\end{lstlisting}
Note that the code is almost identical to the one in \S~\ref{sec:amrex0}.
Some comments:
\begin{itemize}
\item The iterator now takes an extra argument to turn on tiling
(set to {\tt true}).  There is another interface fo {\tt MFIter}
that can take an {\tt IntVect} that explicitly gives the tile size
in each coordinate direction.

If we don't explictly specify the tile size at the loop, then the
runtime parameter {\tt fabarray} {\tt .mfiter\_tile\_size} can be used to set it
globally.

\item {\tt .validBox()} has the same meaning as in the non-tile approach,
so we don't use it.  Instead, we use {\tt .tilebox()} to get the
{\tt Box} (and corresponding {\tt lo} and {\tt hi}) for the {\em
current tile}, not the entire data region.
\end{itemize}

Let us consider an example.  Suppose there are four boxes---see
Figure~\ref{fig:domain-tiling}.
\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{domain-tile}
\caption{\label{fig:domain-tiling} A simple domain showing 4
  {\tt Box}es labeled 0--3, and their tiling regions (dotted lines)}
\end{figure}
%
The first box is divided into 4 logical tiles, the second and third
are divided into 2 tiles each (because they are small), and the fourth
into 4 tiles.  So there are 12 tiles in total.  The difference between
the tiling and non-tiling version are then:
\begin{itemize}
\item In the tiling version,
the loop body will be run 12 times.  Note that {\tt tilebox} is
different for each tile, whereas {\tt fab} might be referencing the
same object if the tiles belong to the same box.

\item In the non-tiling
version (by constructing {\tt MFIter} without the optional second
argument or setting to {\tt false}), the loop body will be run 4 times
because there are four boxes, and a call to {\tt mfi.tilebox()} will
return the traditional {\tt validbox}.  The non-tiling case is
essentially having one tile per box.
\end{itemize}
 
Tiling provides us the opportunity of a coarse-grained approach for
OpenMP.  Threading can be turned on by inserting the following line
above the {\tt for (MFIter...)} line.
\begin{lstlisting}
  #pragma omp parallel
\end{lstlisting}
Assuming four threads are used in the above example, thread 0 will
work on 3 tiles from the first box, thread 1 on 1 tile from the first
box and 2 tiles from the second box, and so forth.  Note that 
OpenMP can be used even when tiling is turned off.  In that case, the
OpenMP granularity is at the box level (and good performance would need
many boxes per MPI task).

While it is possible that, independent of whether or not tiling is on, OpenMP
threading could also be started within the function/loop called inside the
{\tt MFIter} loop, rather than at the {\tt MFIter} loop level, this
is not the approach taken in \iamr.

The tile size for the three spatial dimensions can be set by a
parameter, e.g., {\tt fabarray.mfiter\_tile\_size = 1024000 8 8}.  A
huge number like {\tt 1024000} will turn off tiling in that direction.
As noted above, the {\tt MFIter} constructor can also take an explicit
tile size: {\tt MFIter(mfi(mf,IntVect(128,16,32)))}.

Note that tiling can naturally transition from all threads working
on a single box to each thread working on a separate box as the boxes
coarsen (e.g., in multigrid).

The {\tt MFIter} class provides some other useful functions:
\begin{lstlisting}
 mfi.validbox()       : The same meaning as before independent of tiling.
 mfi.growntilebox(int): A grown tile box that includes ghost cells at box
                        boundaries only.  Thus the returned boxes for a
                        Fab are non-overlapping.
 mfi.nodaltilebox(int): Returns non-overlapping edge-type boxes for tiles.
                        The argument is for direction.
 mfi.fabbox()         : Same as mf[mfi].box().
\end{lstlisting}

Finally we note that tiling is not always desired or better.  This
traditional fine-grained approach coupled with dynamic scheduling is
more appropriate for work with unbalanced loads, such as chemistry
burning in cells by an implicit solver.  Tiling can also create extra
work in the ghost cells of tiles.


\subsection{Practical Details in Working with Tiling}

It is the responsibility of the coder to make sure that the routines within
a tiled region are safe to use with OpenMP.  In particular, note that:
\begin{itemize}
\item tile boxes are non-overlapping
\item the union of tile boxes completely cover the valid region of the fab
\item Consider working with a node-centered MultiFab, {\tt ugdnv}, and a
cell=centered MultiFab, {\tt s}:
  \begin{itemize}

  \item with {\tt mfi(s)}, the tiles are based on the cell-centered
  index space.  If you have an $8\times 8$ box, then and 4 tiles, then
  your tiling boxes will range from $0\rightarrow 3$, $4\rightarrow
  7$.

  \item with {\tt mfi{ugdnv}}, the tiles are based on nodal indices,
  so your tiling boxes will range from $0\rightarrow 3$, $4\rightarrow 8$.

  \end{itemize}  
\item When updating routines to work with tiling, we need to understand
the distinction between the index-space of the entire box (which
corresponds to the memory layout) and the index-space of the tile.
Inside the MFIter loop, make sure to only update over
  the tile region, not for the entire box.

\end{itemize}


