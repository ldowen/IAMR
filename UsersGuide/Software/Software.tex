\section{Code structure}

The code structure in the {\tt IAMR/} directory is as follows:
\begin{itemize}

\item {\tt Source/}: source code

\item {\tt Exec/}: various problem run directories, including:
  \begin{itemize}
  \item {\tt run2d/}
  \item {\tt run3d/}
  \item {\tt eb\_run2d/}
  \item {\tt eb\_run3d/}
  \item {\tt run\_2d\_particles/}
  \end{itemize}

\item {\tt UsersGuide/}: you're reading this now!

\end{itemize}


\section{An Overview of \iamr}

\iamr\ is built upon the \amrex\ C++ framework.  This provides
high-level classes for managing an adaptive mesh refinement simulation,
including the core data structures required in AMR calculations.
Since \iamr\ leaverages heavily from the \amrex\ library,
it's documentation (at https://amrex-codes.github.io/amrex/docs\_html/index.html)
is a useful resource in addition to this User's Guide.

The \iamr\ simulation begins in {\tt IAMR/Source/main.cpp} where an instance
of the \amrex\ {\tt Amr} class is created:
\begin{lstlisting}
  Amr* amrptr = new Amr;
\end{lstlisting}
The initialization, including calling a problem's {\tt initdata()}
routine and refining the base grid occurs next through
\begin{lstlisting}
  amrptr->init(strt_time,stop_time);
\end{lstlisting}
And then comes the main loop over coarse timesteps until the
desired simulation time is reached:
\begin{lstlisting}
  while ( amrptr->okToContinue()                            &&
         (amrptr->levelSteps(0) < max_step || max_step < 0) &&
         (amrptr->cumTime() < stop_time || stop_time < 0.0) )

  {
     //
     // Do a timestep.
     //
     amrptr->coarseTimeStep(stop_time);
  }
\end{lstlisting}
This uses the \amrex\ machinery to do the necessary subcycling in time,
including synchronization between levels, to advance the level hierarchy
forward in time.  

%\subsection{Geometry class}

%\subsection{ParmParse class}

%\subsection{IAMR Data Structures}

\subsubsection{State Data}

The {\tt StateData} class structure defined by \amrex\ is the data container
used to store the field data associated with the state on a single AMR level
during an \iamr\ run.  The entire state consists of a dynamic union, or hierarchy, of
nested {\tt StateData} objects.  Periodic regrid operations modify the hierarchy,
changing the shape of the data containers at the various levels according to
user-specified criteria; new {\tt StateData} objects are created
for the affected levels, and are filled with the ``best'' (finest) available 
data at each location. Instructions for building and managing {\tt StateData} are
encapsulated in the \amrex\ class, {\tt StateDescriptor}; as discussed later,
a {\tt StateDescriptor} will be created for each type of state field, and 
will include information about data centering, required grow cells, and
instructions for transferring data between AMR levels during various synchronization
operations.

In {\tt IAMR/Source/NavieStokesBase.H}, the {\tt enum} {\tt StateType} defines the
different state descriptors for \iamr.  These are setup during the
run by code in {\tt NS\_setup.cpp}, and include (but are not limited to):
\begin{itemize}
\item {\tt State\_Type}: the cell-centered density, velocity, and other scalars (tracers)
\item {\tt Press\_Type}: the node-centered dynamic pressure field.
\item {\tt Divu\_Type}: Stores the right-hand-side of the constraint 
(only matters for low Mach flows when this is nonzero).
\item {\tt Dsdt\_Type}: Stores the time-derivative of the right-hand-side of the constraint 
(only matters for low Mach flows when this is nonzero).
\end{itemize}

Each {\tt StateData} object has two {\tt MultiFabs}, one each for 
old and new times, and can provide an interpolated copy of the state at any time between the two.
Alternatively, can also access the data containers directly, for instance:
\begin{lstlisting}
MultiFab& S_new = get_new_data(State_Type);
\end{lstlisting}
gets a pointer to the multifab containing the hydrodynamics state data
at the new time (here {\tt State\_Type} is the {\tt enum} defined in 
{\tt NavierStokesBase.H}) (note that the class {\tt NavierStokes} 
is a derived classes of {\tt NavierStokesBase}).

{\tt MultiFab} data is distributed in space at the granularity of 
each {\tt Box} in its {\tt BoxArray}.  We iterate over {\tt MultiFab}s using a special
iterator, {\tt MFIter}, which knows about the locality of the data---only the boxes owned by the
processor will be included in the loop on each processor.  An example loop
(taken from code in {\tt NavierStokesBase.cpp}):
\begin{lstlisting}
    //
    // Fill rho at half time
    //
#ifdef _OPENMP
#pragma omp parallel if (Gpu::notInLaunchRegion())
#endif
    for (MFIter mfi(rho_half,TilingIfNotGPU()); mfi.isValid(); ++mfi)
    {
        const Box& bx = mfi.growntilebox();

        // half time
        auto const& rho_h = rho_half.array(mfi);
        // previous time
        auto const& rho_p = rho_ptime.array(mfi);
        // current time
        auto const& rho_c = rho_ctime.array(mfi);

        amrex::ParallelFor(bx, [rho_h, rho_p, rho_c] 
        AMREX_GPU_DEVICE(int i, int j, int k) noexcept
        {
           rho_h(i,j,k) = 0.5 * (rho_p(i,j,k) + rho_c(i,j,k));
        });
    }
}
\end{lstlisting} 
Here, {\tt ++mfi} iterates to the next {\tt FArrayBox} owned by the {\tt MultiFab}, 
and {\tt mfi.isValid()} returns {\tt false} after we've reached 
the last box contained in the {\tt MultiFab}, terminating the loop.
{\tt rho\_half.array(mfi)} creates an object for accessing {\tt FArrayBox} data in
a more array like manner using {\tt operator()}
(more details are in the \amrex\ documentation at
https://amrex-codes.github.io/amrex/docs\_html/Basics.html\#sec-basics-array4 ).

Here {\tt ParallelFor} takes two arguments. The first argument is a Box specifying the iteration index space, and the second argument is a C++ lambda function that works on cell (i,j,k). Variables rho\_half, rho\_ptime and rho\_ctime in the lambda function are captured by value from the enclosing scope. The code above is performance portable. It works with and without GPU support. When \iamr\ is built with GPU support (USE\_CUDA=TRUE), AMREX\_GPU\_DEVICE indicates that the lambda function is a device function and ParallelFor launches a GPU kernel to do the work. When it is built without GPU support, AMREX\_GPU\_DEVICE has no effects whatsoever. It should be emphasized that ParallelFor does not start an OpenMP parallel region. The OpenMP parallel region will be started by the pragma above the MFIter loop if it is built with OpenMP and without enabling GPU (USE\_OMP=TRUE and USE\_CUDA=TRUE are not compatible). Tiling is turned off if GPU is enabled so that more parallelism is exposed to GPU kernels. Also note that when tiling is off, tilebox returns validbox.
(more details are in the \amrex\ documentation at
https://amrex-codes.github.io/amrex/docs\_html/GPU.html\#sec-gpu-for ).


\section{Setting Up Your Own Problem}

To define a new problem, we create a new inputs file in 
a run directory and modify
{\tt IAMR/Source/prob/prob\_init.cpp} accordingly.
The simplest way to get started is to copy the inputs files from an existing
problem.  Here we describe how to customize your problem.

There are several files involved in setting up an \iamr\ problem. It's possible to
create your own new setup by only touching the first of these ({\tt prob\_initData()})
and changing parameters through the inputs file (see section \ref{sec:inputs}).
Here we list the most relevant problem
setup files and thier purpose. If you need further help setting up your problem, please
contact us.
\begin{itemize}

\item {\tt prob\_initData()}:
  Read in initial conditions and problem parameters from the inputs file,
  and initialize the state data (velocity, density, etc.).

\item {\tt NS\_error.cpp}: Define the error estimation criteria used for tagging cells for
  refinement.
  More details in section \label{sec:tagging}

\item {\tt NS\_setup.cpp}: Declare state and derived variables.
  Specify how to fill grow cells for each state or derived variable.
  More details in sections \ref{sec:boundaries}

\item {\tt NS\_derive.cpp}: Define derived variables. 
  More details in sections \ref{sec:derivedVariables}  

\item {\tt NS\_BC.H}: Define the mapping from physical boundary conditions (e.g. outflow)
  to mathematical (e.g. first order extrapolation from last interior cell).
  More details in section \ref{sec:physicalBCs}

\item {\tt NS\_bcfill.H}:
  Define the boundary filling functions for external Dirichlet (i.e. user supplied)
  boundary conditions. Constant Dirichlet conditions can be specified in the
  inputs file without needing to alter  {\tt NS\_bcfill.H}.
  More details in section \ref{sec:physicalBCs}
  
\end{itemize}


\section{Boundaries}
\label{sec:boundaries}
In \amrex, we are primarily concerned with enabling structured-grid
computations.  A key aspect of this is the use of ``grow'' cells
around the ``valid box'' of cells over which we wish to apply stencil operations.
Grow cells, filled properly, are conveniently located temporary 
data containers that allow us to separate the steps of data preparation
(including communication, interpolation, or other complex manipulation)
from stencil application.  The steps that are required to fill grow cells
depends on where the cells ``live'' in the computational domain.

\subsection{Boundaries Between Grids and Levels}
Most of our state data is cell-centered, and often the grow cells are
as well.  When the cells lie directly over cells of a neighboring box
at the same AMR refinement level, these are ``fine-fine'' cells, and are
filled by direct copy (including any MPI communication necessary to enable
that copy).  Note that fine-fine boundary also include grow cells that
cover valid fine cells through a periodic boundary.

When the boundary between valid and grow cells is coincident
with a coarse-fine boundary, these coarse-fine grow cells will hold cell-centered 
temporary data that generated by interpolation (in space and time) of the
underlying coarse data.  This operation requires auxiliary metadata to define 
how the interpolation is to be done, in both space and time.  Importantly,
the interpolation also requires that coarse data be well-defined over
a time interval that brackets the time instant for which we are evaluating
the grow cell value  -- this places requirements on how the time-integration 
of the various AMR levels are sequenced relative to eachother.
In \amrex, the field data associated with the system state, as well as the metadata
associated with inter-level transfers, is bundled (encapsulated) in
a class called ``StateData''.  The metadata 
is defined in {\tt NS\_setup.cpp} -- search for
{\tt cell\_cons\_interp}, for example -- which is ``cell conservative
interpolation'', i.e., the data is cell-based (as opposed to node-based
or edge-based) and the interpolation is such that the average of the
fine values created is equal to the coarse value from which they came.
(This wouldn't be the case with straight linear interpolation, for
example.)  A number of interpolators are provided with \amrex\ and 
user-customizable ones can be added on the fly.

\subsection{Physical Boundaries}
\label{sec:physicalBCs}
The last type of grow cell exists at physical boundaries.  These are special for 
a couple of reasons.  First, the user must explicitly specify how they are to be
filled, consistent with the problem being run.  \amrex\ provides a number of 
standard condition types typical of PDE problems (reflecting, extrapolated, etc),
and a special one that indicates external Dirichlet. In the case of Dirichlet,
the user supplies data to fill grow cells.

\iamr\ provides the ability to specify constant Dirichlet BCs 
in the inputs file (see section \ref{sec:dirichlet}).
Users can create more complex Dirichlet boundary condtions by writing
their own fill function in {\tt NS\_bcfill.H}, then using that function to create
an {\tt amrex::StateDescriptor::BndryFunc} object and specifying which variables
will use it in {\tt NS\_setup.cpp}. 

It is important to note that external Dirichlet boundary data is to be specified as 
if applied on the face of the cell bounding the domain, even for cell-centered
state data. For cell-centered data, the array passed into the 
boundary condition code is filled with cell-centered values in the valid 
region and in fine-fine, and coarse-fine grow cells.  Additionally, grow cells 
for standard extrapolation and reflecting boundaries are pre-filled.  The 
differential operators throughout \iamr\ are aware of the special boundaries
that are Dirichlet and wall-centered, and the stencils are adjusted accordingly.

For convenience, \iamr\ provides a limited set of mappings from a physics-based boundary condition
specification to a mathematical one that the code can apply.  This set can be extended
by adjusting the corresponding translations in {\tt NS\_BC.H}, but, by default, includes 
(See {\tt AMReX/Src/Base/AMReX\_BC\_TYPES.H} for more detail):
\begin{itemize}
\item {\it Outflow}:
  \begin{itemize}
    \item velocity: {\tt FOEXTRAP}
    \item temperature: {\tt FOEXTRAP}
    \item scalars: {\tt FOEXTRAP}
  \end{itemize}
  
\item {\it No Slip Wall with Adiabatic Temp}:
  \begin{itemize}
  \item velocity: {\tt EXT\_DIR}, $u=v=0$
  \item temperature: {\tt REFLECT\_EVEN}, $dT/dt=0$
  \item scalars: {\tt HOEXTRAP}
  \end{itemize}

%% \item {\it No Slip Wall with Fixed Temp}:
%%   \begin{itemize}
%%   \item velocity: {\tt EXT\_DIR}, $u=v=0$
%%   \item temperature: {\tt EXT\_DIR}
%%   \item scalars: {\tt HOEXTRAP}
%%   \end{itemize}
    
\item {\it Slip Wall with Adiabatic Temp}:
  \begin{itemize}
  \item velocity: {\tt EXT\_DIR}, $u_n=0$; {\tt HOEXTRAP}, $u_t$
  \item temperature: {\tt REFLECT\_EVEN}, $dT/dn=0$
  \item scalars: {\tt HOEXTRAP}
  \end{itemize}
  
%% \item {\it Slip Wall with Fixed Temp}:
%%   \begin{itemize}
%%   \item velocity: {\tt EXT\_DIR}, $u_n=0$
%%   \item temperature: {\tt EXT\_DIR}
%%   \item scalars: {\tt HOEXTRAP}
%%   \end{itemize}

\end{itemize}

The keywords used above are defined:
\begin{itemize}
\item {\tt INT\_DIR}: data taken from other grids or interpolated

\item {\tt EXT\_DIR}: data specified on EDGE (FACE) of bndry

\item {\tt HOEXTRAP}: higher order extrapolation to EDGE of bndry

\item {\tt FOEXTRAP}: first order extrapolation from last cell in interior

\item {\tt REFLECT\_EVEN}: $F(-n) = F(n)$ true reflection from interior cells

\item {\tt REFLECT\_ODD}: $F(-n) = -F(n)$ true reflection from interior cells
\end{itemize}


%  -------------------
  %% \item {\tt bc()}: an array that holds the type of boundary conditions
  %%   to enforce at the physical boundaries.

  %%   Sometimes it appears of the form {\tt bc(:,:)} and sometimes
  %%   {\tt bc(:,:,:)}---the last index of the latter holds the variable
  %%   index, i.e., density, velocity, etc.

  %%   The first index is the coordinate direction and the second index
  %%   is the domain face ({\tt 1} is low, {\tt 2} is hi), so {\tt
  %%   bc(1,1)} is the lower $x$ boundary type, {\tt bc(1,2)} is
  %%   the upper $x$ boundary type, {\tt bc(2,1)} is the lower
  %%   $y$ boundary type, etc.

  %%   To interpret the array values, we test against the quantities
  %%   defined in {\tt AMReX\_BC\_TYPES.H} included in each subroutine,
  %%   for example, {\tt EXT\_DIR}, {\tt FOEXTRAP}, $\ldots$.  The
  %%   meaning of these are explained below.
  %%   %-------------------------


\section{Derived Variables}
\label{sec:derivedVariables}

\iamr\ has the ability to created new variables derived from the state variables.
A few derived variables are provided with \iamr, which can be used as examples for
creating user defined derived variables.
Users create derived variables by adding a function to create them in
{\tt NS\_derive.H} and {\tt NS\_derive.cpp}, and then adding the variable to the
{\tt derive\_lst} in {\tt NS\_setup.cpp}.

Access to the derived variable is through one of two {\tt amrex:AmrLevel} functions
(which are inherited by {\tt NavierStokesBase} and {\tt NavierStokes}):
\begin{lstlisting}
    /**
    * \brief Returns a MultiFab containing the derived data for this level.
    * The user is responsible for deleting this pointer when done
    * with it.  If ngrow>0 the MultiFab is built on the appropriately
    * grown BoxArray.
    */
    virtual std::unique_ptr<MultiFab> derive (const std::string& name,
					      Real               time,
					      int                ngrow);
    /**
    * \brief This version of derive() fills the dcomp'th component of mf
    * with the derived quantity.
    */
    virtual void derive (const std::string& name,
                         Real               time,
                         MultiFab&          mf,
                         int                dcomp);
\end{lstlisting}

As an example, {\tt mag\_vort} is a derived variable provided with \iamr, which
returns the magnitude of the vorticity of the flow.
A multifab filled with the magnitude of the vorticity can be obtained via
\begin{lstlisting}
  std::unique_ptr<MultiFab> vort;
  vort = derive(mag_vort, time, ngrow);
  //
  // do something with vorticity...
  //
  vort.reset();  
\end{lstlisting}


\subsection{The {\tt FillPatchIterator}}

A {\tt FillPatchIterator} is a \amrex\ object tasked with the job of
filling rectangular patches of state data, possibly including grow cells,
and, if so, utilizing all the metadata  discussed above that is provided by
the user.  Thus, a {\tt FillPatchIterator} can only be constructed on
a fully registered {\tt StateData} object, and is the preferred 
process for filling grown platters of data prior to most stencil 
operations (e.g., explicit advection operators, which may require 
several grow cells).  It should be mentioned that a {\tt FillPatchIterator}
fills temporary data via copy operations, and therefore does not
directly modify the underlying state data.  In the code, if the state
is modified (e.g., via an advective ``time advance'', the new data
must be copied explicitly back into the {\tt StateData} containers.

Use of {\tt FillPatchIterator} as an iterator has been depreciated in favor
of {\tt MFIter}, which supports tiling (see section \ref{sec:parallel}).
However, \iamr\ continues to use
{\tt FillPatchIterator} for creating temporaries with filled grow cells.

For example, the following code demonstrates the calling sequence to
create and use a {\tt FillPatchIterator} for preparing a rectangular patch of 
data that includes the ``valid region'' plus {\tt NUM\_GROW} grow cells.  Here,
the valid region is specified as a union of rectangular boxes making up the 
box array underlying the {\tt MultiFab S\_new}, and {\tt NUM\_GROW} cells are 
added to each box in all directions to create the temporary patches to
be filled.  

\begin{lstlisting}
  FillPatchIterator fpi(*this, S_new, NUM_GROW,
                        time, State_Type, strtComp, NUM_STATE);
  // Get a reference to the temporary platter of grown data
  MultiFab& S = fpi.get_mf();
\end{lstlisting}
Here the {\tt FillPatchIterator} fills the patch 
with data of type ``{\tt State\_Type}'' at time ``{\tt time}'',
starting with component {\tt strtComp} and including a total of
{\tt NUM\_STATE} components. When the {\tt FillPatchIterator} goes out of scope, it 
and the temporary data platters are destroyed (though much of the 
metadata generated during the operation is cached internally
for performance).  Notice that since {\tt NUM\_GROW} can be any
positive integer (i.e., that the grow region can extend over an arbitrary 
number of successively coarser AMR levels), this key operation can hide an
enormous amount of code and algorithm complexity.

\section{Parallel I/O}

Both checkpoint files and plotfiles are actually folders containing
subfolders: one subfolder for each level of the AMR hierarchy.
The fundamental data structure we read/write to disk is a {\tt MultiFab},
which is made up of multiple FAB's, one FAB per grid.  Multiple
{\tt MultiFab}s may be written to each folder in a checkpoint file.
{\tt MultiFab}s of course are shared across CPUs; a single {\tt MultiFab} may be
shared across thousands of CPUs.  Each CPU writes the part of the
{\tt MultiFab} that it owns to disk, but they don't each write to their own
distinct file.  Instead each {\tt MultiFab} is written to a runtime
configurable number of files N (N can be set in the inputs file as the
parameter {\tt amr.checkpoint\_nfiles} and {\tt amr.plot\_nfiles}; the
default is 64).  That is to say, each {\tt MultiFab} is written to disk
across at most N files, plus a small amount of data that gets written
to a header file describing how the file is laid out in those N files.

What happens is $N$ CPUs each opens a unique one of the $N$ files into
which the {\tt MultiFab} is being written, seeks to the end, and writes
their data.  The other CPUs are waiting at a barrier for those $N$
writing CPUs to finish.  This repeats for another $N$ CPUs until all the
data in the {\tt MultiFab} is written to disk.  All CPUs then pass some data
to CPU {\tt 0} which writes a header file describing how the {\tt MultiFab} is
laid out on disk.

We also read {\tt MultiFab}s from disk in a ``chunky'' manner, opening only $N$
files for reading at a time.  The number $N$, when the {\tt MultiFab}s were
written, does not have to match the number $N$ when the {\tt MultiFab}s are
being read from disk.  Nor does the number of CPUs running while
reading in the {\tt MultiFab} need to match the number of CPUs running when
the {\tt MultiFab} was written to disk.

Think of the number $N$ as the number of independent I/O pathways in
your underlying parallel filesystem.  Of course a ``real'' parallel
filesytem should be able to handle any reasonable value of $N$.  The
value {\tt -1} forces $N$ to the number of CPUs on which you're
running, which means that each CPU writes to a unique file, which can
create a very large number of files, which can lead to inode issues.

